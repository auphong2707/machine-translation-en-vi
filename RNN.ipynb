{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries used for this shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\sy quan\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sy quan\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sy quan\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sy quan\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sy quan\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sy quan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Sy Quan\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets evaluate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sy Quan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import datasets\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import Dict, Iterable, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case we need to use datasets' utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset('csv', data_files={'train': 'data/train.csv', 'test': 'data/test.csv', 'valid': 'data/valid.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datas, valid_datas, test_datas = (\n",
    "#     dataset[\"train\"],\n",
    "#     dataset[\"valid\"],\n",
    "#     dataset[\"test\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "valid_data = pd.read_csv('data/valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only take data from source: 'OpenSubtitles v2018'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datas = train_datas.filter(lambda example: example['source'] == 'OpenSubtitles v2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['source'] == 'OpenSubtitles v2018']\n",
    "test_data = test_data[test_data['source'] == 'OpenSubtitles v2018']\n",
    "valid_data = valid_data[valid_data['source'] == 'OpenSubtitles v2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datas = train_datas.shuffle(seed=42).select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampled = train_data.sample(n=50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first 2 columns from the dataset (english and vietnamese, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_data_sampled['en']\n",
    "train_vi = train_data_sampled['vi']\n",
    "\n",
    "test_en = test_data['en']\n",
    "test_vi = test_data['vi']\n",
    "\n",
    "valid_en = valid_data['en']\n",
    "valid_vi = valid_data['vi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'vi', 'source'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(train_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2269779                       There are thousands of glens.\n",
       "2674860                               I pushed you in that.\n",
       "2541945                 So he wouldn't lie or use an alias.\n",
       "1865205    I remember identifying you after the exhumation.\n",
       "831567              This is where you've been for 20 years?\n",
       "Name: en, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is used to turn a string into a list of tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. We'll start talking about the sentences being a sequence of tokens from now, instead of saying they're a sequence of words. What's the difference? Well, \"good\" and \"morning\" are both words and tokens, but \"!\" is not a word. We could say \"!\" is punctuation, but the term token is more general and covers: words, punctuation, numbers and any special symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps with special suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_suffix(word):\n",
    "    # Check for \"n't\" suffix (negative contractions)\n",
    "    if re.search(r\"n't$\", word):\n",
    "        return [word]  # Return the whole word as a single token\n",
    "    \n",
    "    # Check for specific suffixes ('d, 've, 's, 'm, 'll, 're)\n",
    "    match = re.search(r\"(.*)('d|'ve|'s|'m|'ll|'re)$\", word)\n",
    "    if match:\n",
    "        return [match.group(1), match.group(2)]  # Return the part before and the suffix\n",
    "\n",
    "    # Check for a word ending with just '\n",
    "    match = re.search(r\"(.*)('$)$\", word)\n",
    "    if match:\n",
    "        return [match.group(1), match.group(2)]  # Return the part before and just '\n",
    "\n",
    "    # Check for apostrophe + other characters (return <unk>)\n",
    "    if re.search(r\"(.*)('\\w+)$\", word):\n",
    "        return [\"<unk>\"]  # Return <unk>\n",
    "\n",
    "    return [word]  # Return the word unchanged if no match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function tokenizes a line of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'can', 'now', 'install', 'torchtext', '(', 'but', 'i', \"can't\", ')', 'and', 'this', \"'s\", 'for', 'checking', 'the', 'split_suffix', 'functions', \"'\", '<unk>', '!']\n"
     ]
    }
   ],
   "source": [
    "patterns = [r\"\\\"\", r\"\\.\", r\"<br \\/>\", r\",\", r\"\\(\", r\"\\)\", r\"\\!\", r\"\\?\", r\"\\;\", r\"\\:\", r\"\\s+\"]\n",
    "replacements = [\"\", \" . \", \" \", \" , \", \" ( \", \" ) \", \" ! \", \" ? \", \" \", \" \", \" \"]\n",
    "patterns_dict = list((re.compile(p), r) for p, r in zip(patterns, replacements))\n",
    "\n",
    "def get_tokenizer_line(line):\n",
    "    \"\"\"\n",
    "    Basic normalization for a line of text.\n",
    "    Normalization includes\n",
    "    - lowercasing\n",
    "    - complete some basic text normalization for English words as follows:\n",
    "        add spaces before and after '\\''\n",
    "        remove '\\\"',\n",
    "        add spaces before and after '.'\n",
    "        replace '<br \\/>'with single space\n",
    "        add spaces before and after ','\n",
    "        add spaces before and after '('\n",
    "        add spaces before and after ')'\n",
    "        add spaces before and after '!'\n",
    "        add spaces before and after '?'\n",
    "        replace ';' with single space\n",
    "        replace ':' with single space\n",
    "        replace multiple spaces with single space\n",
    "\n",
    "    Returns a list of tokens after splitting on whitespace.\n",
    "    \"\"\"\n",
    "    line = line.lower()\n",
    "    for pattern_re, replaced_str in patterns_dict:\n",
    "        line = pattern_re.sub(replaced_str, line)\n",
    "    result = [split_suffix(word) for word in line.split()]\n",
    "    flattened_result = [token for sublist in result for token in sublist]\n",
    "    return flattened_result\n",
    "\n",
    "print(get_tokenizer_line(\"You can now install TorchText (but I can't) and this's for checking the split_suffix functions' i'dk!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function tokenizes text (in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!'],\n",
       " ['this', 'is', 'a', 'test', '.']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokenizer(text):\n",
    "    tokens = []\n",
    "    for line in text:\n",
    "        tokens.append(get_tokenizer_line(line))\n",
    "    return tokens\n",
    "        \n",
    "get_tokenizer([\"You can now install TorchText using pip!\", \"This is a test.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, max_length, sos_token, eos_token):\n",
    "    \"\"\"\n",
    "    Apply the tokenizer to all of the examples in each data split, as well as some other processing.\n",
    "\n",
    "    This function takes in an example from the dataset, applies the get_tokenizer function, trims the list of tokens to a maximum length, \n",
    "    and then appends the start of sequence and end of sequence tokens to the beginning and end of the list of tokens.\n",
    "    \"\"\"\n",
    "    en_tokens = get_tokenizer(example[\"en\"])[:max_length]\n",
    "    vi_tokens = get_tokenizer(example[\"vi\"])[:max_length]\n",
    "    en_tokens = [[sos_token] + token + [eos_token] for token in en_tokens]\n",
    "    vi_tokens = [[sos_token] + token + [eos_token] for token in vi_tokens]\n",
    "    return {\"en_tokens\": en_tokens, \"vi_tokens\": vi_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_tokens': [['<sos>',\n",
       "   'there',\n",
       "   'are',\n",
       "   'thousands',\n",
       "   'of',\n",
       "   'glens',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>', 'i', 'pushed', 'you', 'in', 'that', '.', '<eos>'],\n",
       "  ['<sos>',\n",
       "   'so',\n",
       "   'he',\n",
       "   \"wouldn't\",\n",
       "   'lie',\n",
       "   'or',\n",
       "   'use',\n",
       "   'an',\n",
       "   'alias',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>',\n",
       "   'i',\n",
       "   'remember',\n",
       "   'identifying',\n",
       "   'you',\n",
       "   'after',\n",
       "   'the',\n",
       "   'exhumation',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>',\n",
       "   'this',\n",
       "   'is',\n",
       "   'where',\n",
       "   'you',\n",
       "   \"'ve\",\n",
       "   'been',\n",
       "   'for',\n",
       "   '20',\n",
       "   'years',\n",
       "   '?',\n",
       "   '<eos>']],\n",
       " 'vi_tokens': [['<sos>',\n",
       "   'có',\n",
       "   'hàng',\n",
       "   'ngàn',\n",
       "   'thung',\n",
       "   'lũng',\n",
       "   'như',\n",
       "   'thế',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>', 'tớ', 'đẩy', 'cậu', 'đi', 'bằng', 'cái', 'đó', '.', '<eos>'],\n",
       "  ['<sos>',\n",
       "   'nên',\n",
       "   'hắn',\n",
       "   'sẽ',\n",
       "   'không',\n",
       "   'nói',\n",
       "   'dối',\n",
       "   'hay',\n",
       "   'dùng',\n",
       "   'bí',\n",
       "   'danh',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>',\n",
       "   'anh',\n",
       "   'nhớ',\n",
       "   'lại',\n",
       "   'lúc',\n",
       "   'nhận',\n",
       "   'dạng',\n",
       "   'em',\n",
       "   'khi',\n",
       "   'họ',\n",
       "   'đào',\n",
       "   'mộ',\n",
       "   '.',\n",
       "   '<eos>'],\n",
       "  ['<sos>', 'bà', 'đã', 'ở', 'đây', '20', 'năm', 'sao', '?', '<eos>']]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_example(train_data_sampled[:5], 20, \"<sos>\", \"<eos>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're trimming all sequences to a maximum length of 1000000000 tokens, and using *sos* and *eos* as the start and end of sequence tokens, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000_000_000\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"max_length\": max_length,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data_tokenized = tokenize_example(train_data_sampled, **fn_kwargs)\n",
    "valid_data_tokenized = tokenize_example(valid_data, **fn_kwargs)\n",
    "test_data_tokenized = tokenize_example(test_data, **fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 320.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# max_length = 1_000_000_000\n",
    "# sos_token = \"<sos>\"\n",
    "# eos_token = \"<eos>\"\n",
    "\n",
    "# fn_kwargs = {\n",
    "#     \"max_length\": max_length,\n",
    "#     \"sos_token\": sos_token,\n",
    "#     \"eos_token\": eos_token,\n",
    "# }\n",
    "\n",
    "# train_datas_tokenized = train_datas.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "# test_datas_tokenized = test_datas.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "# valid_datas_tokenized = valid_datas.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at some examples, confirming the two new features have been added; both of which are lowercased list of strings with the start/end of sequence tokens appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en_tokens': [['<sos>', 'there', 'are', 'thousands', 'of', 'glens', '.', '<eos>'], ['<sos>', 'i', 'pushed', 'you', 'in', 'that', '.', '<eos>'], ['<sos>', 'so', 'he', \"wouldn't\", 'lie', 'or', 'use', 'an', 'alias', '.', '<eos>'], ['<sos>', 'i', 'remember', 'identifying', 'you', 'after', 'the', 'exhumation', '.', '<eos>'], ['<sos>', 'this', 'is', 'where', 'you', \"'ve\", 'been', 'for', '20', 'years', '?', '<eos>'], ['<sos>', 'look', ',', 'let', \"'s\", 'put', 'some', 'real', 'miles', 'on', 'those', 'bikes', '.', '<eos>'], ['<sos>', '-', 'i', \"don't\", 'know', 'what', 'happened', '.', '<eos>'], ['<sos>', 'comes', 'out', 'to', 'a', 'million', ',', \"doesn't\", 'it', '?', '<eos>'], ['<sos>', 'it', \"'s\", 'not', 'good', 'for', 'a', 'son', 'of', 'pharaoh', 'to', 'see', 'his', 'father', 'humbled', 'by', 'a', 'son', 'of', 'slaves', '.', '<eos>'], ['<sos>', 'it', \"'s\", 'all', 'happening', 'right', 'in', 'front', 'of', 'your', 'eyes', 'over', 'and', 'over', '.', '<eos>'], ['<sos>', 'well', ',', 'they', ',', 'uh--', 'they', 'let', 'him', 'go', ',', 'malvo', '.', '<eos>'], ['<sos>', 'all', 'right', '.', '7', 'years', 'ago', ',', 'dan', \"'s\", 'divorce', 'had', 'just', 'come', 'through', 'and', 'his', 'ex-wife', 'had', 'left', 'his', 'heart', 'in', 'tatters', '.', '<eos>'], ['<sos>', 'in', 'the', 'army', ',', 'they', 'taught', 'us', 'the', 'fastest', 'way', 'to', 'get', 'shot', 'was', 'to', 'fail', 'to', 'clean', 'your', 'weapon', '.', '<eos>'], ['<sos>', '-', 'i', '-', 'i', 'just', 'touched', 'it', '.', '<eos>'], ['<sos>', 'they', 'had', 'way', 'too', 'many', 'eggs', 'in', 'one', 'basket', 'on', 'that', 'day', '.', '<eos>'], ['<sos>', 'all', 'injector', 'subsystems', 'aboard', 'the', 'collector', 'are', 'confirmed', 'off-line', '.', '<eos>'], ['<sos>', \"haven't\", 'had', 'anything', 'in', 'over', 'four', 'days', ',', 'and', \"i'm--\", '<eos>'], ['<sos>', 'miss', ',', 'did', 'a', 'strange', 'man', 'just', 'run', 'past', 'here', '?', '<eos>'], ['<sos>', 'i', 'prefer', 'fielding', 'any', 'day', '.', '<eos>'], ['<sos>', '-', 'it', \"'s\", 'time', 'we', 'took', 'a', 'stand', '.', '<eos>'], ['<sos>', 'what', 'would', 'you', 'recommend', 'me', 'to', 'do', ',', 'ali', '?', '<eos>'], ['<sos>', 'could', 'you', 'have', 'a', 'lousier', 'poker', 'face', '?', '<eos>'], ['<sos>', 'so', 'the', 'four', 'marines', 'who', 'were', 'killed', ',', 'they', 'were', 'transporting', 'cash', '?', '<eos>'], ['<sos>', 'we', 'got', 'new', 'players', 'in', 'town', '.', '<eos>'], ['<sos>', 'you', \"'ve\", 'got', '19', 'seconds', 'to', 'clear', 'the', 'cable', '.', '<eos>'], ['<sos>', 'he', \"'ll\", 'tell', 'you', 'everything', 'you', 'want', 'to', 'know', 'about', 'him', '.', '<eos>'], ['<sos>', 'and', 'if', 'you', 'ever', 'do', '.', '.', '.', '<eos>'], ['<sos>', 'you', 'let', 'me', 'help', 'break', 'your', 'heart', '.', '<eos>'], ['<sos>', 'well', ',', 'we', 'darn', 'sure', 'know', 'it', 'and', 'we', \"'re\", 'not', 'home', '.', '<eos>'], ['<sos>', 'i', \"don't\", 'mean', 'the', 'wine', ',', 'dale', '.', '<eos>'], ['<sos>', 'you', 'want', 'to', 'end', 'up', 'like', 'that', 'door', '?', '<eos>'], ['<sos>', 'you', 'have', 'seen', 'me', ',', 'tuco', '.', '<eos>'], ['<sos>', 'this', 'is', 'tactically', 'dangerous', '.', '<eos>'], ['<sos>', 'try', 'not', 'to', 'depend', 'on', 'him', '.', '<eos>'], ['<sos>', '-', 'what', 'did', 'she', 'do', 'that', \"'s\", 'so', 'bad', 'that', 'you', \"won't\", 'talk', 'to', 'her', '?', '<eos>'], ['<sos>', 'long', 'qi', ',', 'you', 'really', 'think', 'you', 'are', 'the', 'king', 'of', 'shanghai', '.', '<eos>'], ['<sos>', 'we', 'went', 'to', 'saldua', \"'s\", 'home', 'this', 'morning', 'and', 'examined', '.', '.', '.', 'his', 'bag', 'of', 'rice', '.', '<eos>'], ['<sos>', 'she', 'saved', 'my', 'life', 'and', 'my', 'daughter', \"'s\", '.', '<eos>'], ['<sos>', 'for', 'a', 'while', ',', 'things', 'seemed', 'to', 'be', 'going', 'along', 'pretty', 'well', ',', 'considering', '.', 'i', 'made', 'a', 'decision', 'to', 'just', '.', '.', '.', 'stay', 'the', 'course', '.', '<eos>'], ['<sos>', '-', 'shall', 'we', '?', '-', 'oh', ',', 'yes', '.', '<eos>'], ['<sos>', 'how', 'did', 'the', 'falcone', 'and', 'maroni', '<eos>'], ['<sos>', 'it', 'is', 'the', 'mustang', ',', 'right', '?', 'ok', ',', 'what', \"'s\", 'the', 'plate', 'number', '?', '<eos>'], ['<sos>', 'okay', ',', 'but', 'if', 'we', 'did', 'this', 'right', ',', 'you', 'could', 'come', 'out', 'of', 'hiding', '.', '<eos>'], ['<sos>', 'i', \"'m\", 'rewriting', 'valkyrie', 'to', 'direct', 'the', 'majority', 'of', 'our', 'strongest', 'units', 'to', 'focus', 'entirely', 'on', 'berlin', '.', '<eos>'], ['<sos>', 'heise', 'and', 'thomas', 'living', 'room', ',', 'study', '.', '<eos>'], ['<sos>', 'bathed', 'in', 'blood', 'and', 'gore', '.', '<eos>'], ['<sos>', '-', 'yeah', ',', 'trying', 'to', 'save', 'the', 'mission', '.', '<eos>'], ['<sos>', 'them', 'pills', 'helping', 'your', 'mama', \"'s\", 'mood', 'any', '?', '<eos>'], ['<sos>', 'when', 'a', 'man', 'is', 'an', 'undercover', 'cop', ',', 'relationships', 'are', 'not', 'what', 'they', 'seem', '.', '<eos>'], ['<sos>', 'your', 'filthy', 'drug', 'money', 'has', 'been', 'transformed', 'into', 'nice', ',', 'clean', ',', 'taxable', 'income', 'brought', 'to', 'you', 'by', 'a', 'savvy', 'investment', 'in', 'a', 'thriving', 'business', '.', '<eos>']], 'vi_tokens': [['<sos>', 'có', 'hàng', 'ngàn', 'thung', 'lũng', 'như', 'thế', '.', '<eos>'], ['<sos>', 'tớ', 'đẩy', 'cậu', 'đi', 'bằng', 'cái', 'đó', '.', '<eos>'], ['<sos>', 'nên', 'hắn', 'sẽ', 'không', 'nói', 'dối', 'hay', 'dùng', 'bí', 'danh', '.', '<eos>'], ['<sos>', 'anh', 'nhớ', 'lại', 'lúc', 'nhận', 'dạng', 'em', 'khi', 'họ', 'đào', 'mộ', '.', '<eos>'], ['<sos>', 'bà', 'đã', 'ở', 'đây', '20', 'năm', 'sao', '?', '<eos>'], ['<sos>', 'chúng', 'ta', 'sẽ', 'đi', 'xe', 'máy', 'trên', 'những', 'dặm', 'đường', 'thật', 'sự', '<eos>'], ['<sos>', '-', 'con', 'không', 'biết', 'chuyện', 'gì', 'đã', 'xảy', 'ra', '<eos>'], ['<sos>', 'một', 'triệu', ',', 'phải', 'không', '?', '<eos>'], ['<sos>', 'không', 'tốt', 'cho', 'một', 'con', 'trai', 'của', 'pharaoh', 'chứng', 'kiến', 'cha', 'mình', 'nhún', 'nhường', 'trước', 'con', 'trai', 'của', 'những', 'nô', 'lệ', '.', '<eos>'], ['<sos>', 'mọi', 'thứ', 'cứ', 'xảy', 'ra', 'trước', 'mắt', 'anh', '.', '<eos>'], ['<sos>', 'họ', ',', 'ừm', '.', '.', '.', 'họ', 'để', 'malvo', 'đi', '.', '<eos>'], ['<sos>', 'được', 'rồi', ',', '7', 'năm', 'trước', ',', 'dan', 'ly', 'dị', 'vợ', '.', '.', '.', 'và', 'cô', 'vợ', 'cũ', 'đã', 'làm', 'tan', 'nát', 'trái', 'tim', 'anh', 'ấy', '<eos>'], ['<sos>', 'trong', 'quân', 'đội', ',', 'họ', 'cho', 'biết', 'nếu', 'không', 'muốn', 'chết', 'sớm', 'thì', 'đừng', 'quên', 'xoáy', 'nóng', '.', '<eos>'], ['<sos>', '-', 'mình-mình', 'chỉ', 'chạm', 'vào', 'nó', '.', '.', '.', '<eos>'], ['<sos>', 'họ', 'tập', 'trung', 'quá', 'nhiều', 'nguồn', 'lực', 'vào', '1', 'chỗ', 'hồi', 'đó', '.', '<eos>'], ['<sos>', 'tất', 'cả', 'các', 'kênh', 'liên', 'lạc', 'trên', 'con', 'tàu', 'đã', 'được', 'ngắt', '.', '<eos>'], ['<sos>', 'hơn', 'bốn', 'ngày', 'nay', 'tôi', 'không', 'có', 'gì', ',', 'và', 'tôi', '.', '.', '.', '<eos>'], ['<sos>', 'thím', 'ơi', 'có', 'người', 'lạ', 'nào', 'chạy', 'qua', 'đây', 'không', 'ạ', '?', '<eos>'], ['<sos>', 'em', 'thích', 'loại', 'fielding', '.', '<eos>'], ['<sos>', '-', 'đây', 'là', 'lúc', 'để', 'chúng', 'ta', 'đứng', 'lên', '.', '<eos>'], ['<sos>', 'anh', 'có', 'ý', 'kiến', 'gì', 'không', ',', 'ali', '?', '<eos>'], ['<sos>', 'ông', 'có', 'thể', 'làm', 'ngơ', 'như', 'thế', 'chứ', '?', '<eos>'], ['<sos>', 'vậy', '4', 'người', 'lính', 'bị', 'giết', ',', 'họ', 'đang', 'vận', 'chuyển', 'tiền', 'sao', '?', '<eos>'], ['<sos>', 'ta', 'có', 'thêm', 'đối', 'thủ', 'mới', 'trong', 'thành', 'phố', 'này', '.', '<eos>'], ['<sos>', 'ethan', ',', 'anh', 'có', '19', 'giây', 'để', 'bỏ', 'dây', 'cáp', 'ra', '.', '<eos>'], ['<sos>', 'ông', 'ấy', 'sẽ', 'nói', 'mọi', 'chuyện', 'mà', 'anh', 'bạn', 'muốn', 'biết', '.', '<eos>'], ['<sos>', 'và', 'nếu', 'anh', 'vẫn', 'cứ', 'đi', 'với', 'lũ', 'bạn', 'anh', '.', '.', '.', '<eos>'], ['<sos>', 'bà', 'để', 'tôi', 'giúp', 'làm', 'bà', 'tan', 'nát', '.', '<eos>'], ['<sos>', 'giờ', 'thì', 'chúng', 'ta', 'nhớ', 'nhà', ',', 'mà', 'vẫn', 'chưa', 'được', 'về', 'nhà', '<eos>'], ['<sos>', 'ý', 'tôi', 'không', 'phải', 'rượu', ',', 'dale', '.', '<eos>'], ['<sos>', 'ngươi', 'có', 'muốn', 'kết', 'thúc', 'như', 'cánh', 'cửa', 'này', 'không', '?', '<eos>'], ['<sos>', 'anh', 'đã', 'gặp', 'tôi', 'rồi', ',', 'tuco', '.', '<eos>'], ['<sos>', 'đây', 'là', 'một', 'kế', 'hoạch', 'nguy', 'hiểm', '.', 'chạy', 'nhanh', 'lên', '.', '<eos>'], ['<sos>', 'mong', 'cô', 'có', 'thể', 'từ', 'bỏ', 'tâm', 'lý', 'dựa', 'dẫm', 'này', '.', '<eos>'], ['<sos>', 'chị', 'ấy', 'đã', 'làm', 'gì', 'khiến', 'cô', 'ko', 'thèm', 'nói', 'chuyện', 'vậy', '?', '<eos>'], ['<sos>', 'long', 'thất', 'cứ', 'tưởng', 'hắn', 'là', 'ông', 'vua', 'ở', 'thượng', 'hải', '.', '<eos>'], ['<sos>', 'sáng', 'nay', 'chúng', 'tôi', 'đã', 'tới', 'nhà', 'saldua', 'và', 'kiểm', 'tra', '.', '.', '.', '.', 'cái', 'túi', 'gạo', '<eos>'], ['<sos>', 'cô', 'ấy', 'đã', 'cứu', 'cuộc', 'đời', 'tôi', 'và', 'con', 'gái', 'tôi', '.', '<eos>'], ['<sos>', 'cómộtthờigian', ',', 'cóvẻnhưmọi', 'thứ', 'sẽ', 'trởnêntốtđẹp', 'một', 'cáchlâudài', ',', 'tôi', 'đã', 'đưa', 'ra', 'một', 'một', 'quyết', 'định', '.', '.', '.', '<eos>'], ['<sos>', '-chúng', 'ta', 'đi', 'được', 'chưa', '?', '<eos>'], ['<sos>', 'làm', 'sao', 'mà', 'gia', 'đình', 'tội', 'phạm', 'của', '<eos>'], ['<sos>', 'ta', 'đang', 'nói', 'về', 'cái', 'xe', 'mustang', 'hả', '?', '<eos>'], ['<sos>', 'anh', 'có', 'thể', 'không', 'phải', 'lẩn', 'trốn', 'nữa', '.', '<eos>'], ['<sos>', 'tôi', 'soạn', 'lại', 'valkyrie', '.', '.', '.', 'và', 'tập', 'trung', 'tại', 'berlin', 'các', 'binh', 'đoàn', 'thiện', 'chiến', 'nhất', '.', '<eos>'], ['<sos>', 'thomas', ',', 'anh', 'vào', 'phòng', 'khách', 'và', 'phòng', 'làm', 'việc', '<eos>'], ['<sos>', 'hắn', 'tắm', 'trong', 'máu', 'và', 'thịt', '.', '<eos>'], ['<sos>', '-', 'yeah', ',', 'vì', 'muốn', 'cứu', 'lấy', 'nhiệm', 'vụ', '.', '<eos>'], ['<sos>', 'những', 'viên', 'thuốc', 'này', 'có', 'giúp', 'gì', 'cho', 'bà', 'mẹ', 'bệnh', 'tật', 'của', 'cháu', 'không', '?', '<eos>'], ['<sos>', 'khi', 'một', 'kẻ', 'là', 'nội', 'gián', ',', 'thì', 'tình', 'cảm', 'không', 'phải', 'thứ', 'anh', 'ta', 'cần', '.', '<eos>'], ['<sos>', 'đống', 'tiền', 'ma', 'túy', 'bẩn', 'đã', 'được', 'chuyển', 'hóa', 'thành', '.', '.', '.', 'nguồn', 'tiền', 'sạch', ',', 'đẹp', 'và', 'có', 'thể', 'đánh', 'thuế', '.', '.', '.', 'có', 'được', 'nhờ', 'đầu', 'tư', 'khôn', 'ngoan', 'và', 'việc', 'kinh', 'doanh', 'phát', 'đạt', '.', '<eos>']]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en_tokens'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_datas_tokenized.select_columns([\"en_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': ['Lost the game, but not the spirit', \"And I did, and it's great.\", 'A sign of surrender. And shame.', 'You seemed to be in the middle of a new disaster every time I turned on the news.', \"♪ Oh, when you're almost there ♪ ♪ Almost home ♪\"], 'vi': ['Thua trận, nhưng không thua ý chí', 'Và anh đã làm được, điều đó thật tuyệt.', 'là dấu hiệu đầu hàng và sự hổ thẹn.', 'Anh trông giống như đang ở tâm của mội đại dịch mỗi lần tôi bật chương trình tin tức.', 'Khi bạn tới gần nơi đó Gần thật gần.'], 'source': ['OpenSubtitles v2018', 'OpenSubtitles v2018', 'OpenSubtitles v2018', 'OpenSubtitles v2018', 'OpenSubtitles v2018'], 'en_tokens': [[['<sos>', 'l', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', ',', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'b', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'p', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 't', '<eos>']], [['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', ',', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '', \"'\", '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', 'a', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'f', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', 'y', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'b', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'l', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'f', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'w', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'v', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'y', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'w', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', '♪', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', ',', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'w', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'y', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '', \"'\", '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'l', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '♪', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '♪', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'l', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'o', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'e', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '♪', '<eos>']]], 'vi_tokens': [[['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'â', '<eos>'], ['<sos>', '̣', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', ',', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ư', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'k', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ô', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'y', '<eos>'], ['<sos>', '́', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '́', '<eos>']], [['<sos>', 'v', '<eos>'], ['<sos>', 'à', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ã', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'l', '<eos>'], ['<sos>', 'à', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ư', '<eos>'], ['<sos>', 'ợ', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', ',', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'ề', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ó', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ậ', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', 'y', '<eos>'], ['<sos>', 'ệ', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', 'l', '<eos>'], ['<sos>', 'à', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'ấ', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'ệ', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ầ', '<eos>'], ['<sos>', 'u', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'à', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'v', '<eos>'], ['<sos>', 'à', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 's', '<eos>'], ['<sos>', 'ự', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ổ', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ẹ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'ô', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'ố', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ư', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'ở', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'â', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', 'ủ', '<eos>'], ['<sos>', 'a', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'ộ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ạ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'd', '<eos>'], ['<sos>', 'ị', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'm', '<eos>'], ['<sos>', 'ỗ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'l', '<eos>'], ['<sos>', 'ầ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'ô', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'b', '<eos>'], ['<sos>', 'ậ', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ư', '<eos>'], ['<sos>', 'ơ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'r', '<eos>'], ['<sos>', 'ì', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'ứ', '<eos>'], ['<sos>', 'c', '<eos>'], ['<sos>', '.', '<eos>']], [['<sos>', 'k', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'b', '<eos>'], ['<sos>', 'ạ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'ớ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'ầ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', 'ơ', '<eos>'], ['<sos>', 'i', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'đ', '<eos>'], ['<sos>', 'ó', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'ầ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', 'h', '<eos>'], ['<sos>', 'ậ', '<eos>'], ['<sos>', 't', '<eos>'], ['<sos>', '<eos>'], ['<sos>', 'g', '<eos>'], ['<sos>', 'ầ', '<eos>'], ['<sos>', 'n', '<eos>'], ['<sos>', '.', '<eos>']]]}\n"
     ]
    }
   ],
   "source": [
    "# print(train_datas_tokenized[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build the vocabulary for the source and target languages. The vocabulary is used to associate each unique token in our dataset with an index (an integer), e.g. \"hello\" = 1, \"world\" = 2, \"bye\" = 3, \"hates\" = 4, etc. When feeding text data to our model, we convert the string into tokens and then the tokens into numbers using the vocabulary as a look up table, e.g. \"hello world\" becomes [\"hello\", \"world\"] which becomes [1, 2] using the example indices given. We do this as neural networks cannot operate on strings, only numerical values.\n",
    "\n",
    "In theory, our vocabulary can be large enough to have an index for every unique token in our dataset. However, what happens if a token exists in our validation and test set, but not in our training set? In that case, we replace the token with an \"unknown token\", denoted by **_unk_**, which is given its own index (usually index zero). All unknown tokens are replaced by **_unk_**, even if the tokens are different, i.e. if the tokens \"gilgamesh\" and \"enkidu\" were both not within our vocabulary, then the string \"gilgamesh hates enkidu\" gets tokenized to [\"gilgamesh\", \"hates\", \"enkidu\"] and then becomes [0, 24, 0] (where \"hates\" has the index 24).\n",
    "\n",
    "Ideally, we want our model to be able to handle unknown tokens by learning to use the context around them to make translations. The only way it can learn that is if we also have unknown tokens in the training set. Hence, when creating our vocabularies with build_vocab_from_iterator, we use the \"min_freq\" to not create an index for tokens which appear less than min_freq times in our training set. In other words, when using the vocabulary, any token which does not appear at least twice in our training set will get replaced by the unknown token index when converting tokens to indices.\n",
    "\n",
    "We also use the \"specials\" argument to pass special tokens. These are tokens which we want to add to the vocabulary but do not necessarily appear in our tokenized examples. These special tokens will appear first in the vocabulary. We've already discussed the unk_token, sos_token, and eos_token. The final special token is the pad_token, denoted by **_pad_**.\n",
    "\n",
    "When inputting sentences into our model, it is more efficient to pass multiple sentences at once (known as a batch), instead of one at a time. The requirement for sentences to be batched together is that they all have to be the same length (in terms of the number of tokens). The majority of our sentences are not the same length, but we can solve this by \"padding\" (adding **_pad_**> tokens) the tokenized version of each sentence in a batch until they all have equal tokens to the longest sentence in the batch. For example, if we had two sentences: \"I love pizza\" and \"I hate music videos\". They would be tokenized to something like: [\"i\", \"love\", \"pizza\"] and [\"i\", \"hate\", \"music\", \"videos\"]. The first sequence of tokens would then be padded to [\"i\", \"love\", \"pizza\", \"**_pad_**\"]. Both sequences could then be converted to indexes using the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of information, but torchtext can handle all the fuss of building the vocabulary. Unfortunately, I can't import torchtext, so the whole process will have to be built manually (Đm torchtext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, max_vocab_size=None, min_freq=1, specials=['<pad>', '<unk>', '<bos>', '<eos>']):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.specials = specials\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "        self.counter = Counter()\n",
    "\n",
    "        # Adding special tokens first\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        for token in self.specials:\n",
    "            self.add_token(token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            idx = len(self.idx2token)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token.append(token)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        # Count frequencies of all tokens\n",
    "        for sentence in sentences:\n",
    "            self.counter.update(sentence)\n",
    "        \n",
    "        # Filter out tokens by frequency and limit vocab size\n",
    "        most_common = self.counter.most_common(self.max_vocab_size) if self.max_vocab_size else self.counter.items()\n",
    "        for token, freq in most_common:\n",
    "            if freq >= self.min_freq and token not in self.token2idx:\n",
    "                self.add_token(token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_index(self, token):\n",
    "        return self.token2idx.get(token, self.token2idx['<unk>'])\n",
    "\n",
    "    def get_token(self, index):\n",
    "        return self.idx2token[index] if index < len(self.idx2token) else '<unk>'\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.get_index(token) for token in sentence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return [self.get_token(index) for index in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 307)\n"
     ]
    }
   ],
   "source": [
    "# Initialize vocabulary\n",
    "en_vocab = Vocabulary(max_vocab_size=1_000_000_000, min_freq=1)\n",
    "vi_vocab = Vocabulary(max_vocab_size=1_000_000_000, min_freq=1)\n",
    "\n",
    "# Build the vocabulary from sentences\n",
    "en_vocab.build_vocab(train_data_tokenized[\"en_tokens\"])\n",
    "vi_vocab.build_vocab(train_data_tokenized[\"vi_tokens\"])\n",
    "\n",
    "print((len(en_vocab), len(vi_vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2, 14, 1, 1, 13, 14, 1, 1, 1, 5, 3]\n",
      "Decoded: ['<bos>', 'i', '<unk>', '<unk>', 'and', 'i', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# Encode a sentence\n",
    "encoded_sentence = en_vocab.encode(['<bos>', \"i\", \"love\", \"learning\", \"and\", \"i\", \"am\", \"gay\", \"!\", \".\", \"<eos>\"])\n",
    "print(\"Encoded:\", encoded_sentence)\n",
    "\n",
    "# Decode the sentence\n",
    "decoded_sentence = en_vocab.decode(encoded_sentence)\n",
    "print(\"Decoded:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.get_index(\"i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a numericalize function which we'll to \"numericalize\" (a fancy way of saying convert tokens to indices) our tokens in each example using the vocabularies and return the result into new \"en_ids\" and \"vn_ids\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(data):\n",
    "    dict = {\"en_ids\": [], \"vi_ids\": []}\n",
    "    for i in range(len(data[\"en_tokens\"])):\n",
    "        en_ids = en_vocab.encode(data[\"en_tokens\"][i])\n",
    "        vi_ids = vi_vocab.encode(data[\"vi_tokens\"][i])\n",
    "        dict[\"en_ids\"].append(en_ids)\n",
    "        dict[\"vi_ids\"].append(vi_ids)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the numericalize_example function to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_numericalized = numericalize(train_data_tokenized)\n",
    "test_data_numericalized = numericalize(test_data_tokenized)\n",
    "valid_data_numericalized = numericalize(valid_data_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking an example, we can see that it has the two new features: \"en_ids\" and \"de_ids\", both a list of integers representing their indices in the respective vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 70, 24, 71, 16, 72, 5, 3]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data_numericalized.values())[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the indices are correct by using the decode method with the corresponding vocabulary on the list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'there', 'are', 'thousands', 'of', 'glens', '.', '<eos>']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.decode(list(train_data_numericalized.values())[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices in the example above are Python integers. They need to be converted to PyTorch tensors in order to use them with PyTorch. Therefore, we will convert them to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(data):\n",
    "    dict = {\"en_ids\": [], \"vi_ids\": []}\n",
    "    for i in range(len(data[\"en_ids\"])):\n",
    "        en_ids = torch.tensor(data[\"en_ids\"][i])\n",
    "        vi_ids = torch.tensor(data[\"vi_ids\"][i])\n",
    "        dict[\"en_ids\"].append(en_ids)\n",
    "        dict[\"vi_ids\"].append(vi_ids)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_numericalized = convert_to_tensors(train_data_numericalized)\n",
    "test_data_numericalized = convert_to_tensors(test_data_numericalized)\n",
    "valid_data_numericalized = convert_to_tensors(valid_data_numericalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this worked by checking an example and seeing the \"en_ids\" and \"de_ids\" features are listed as tensor([...])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': [tensor([ 4, 70, 24, 71, 16, 72,  5,  3]),\n",
       "  tensor([ 4, 14, 73,  8, 15, 21,  5,  3]),\n",
       "  tensor([ 4, 34, 44, 74, 75, 76, 77, 45, 78,  5,  3]),\n",
       "  tensor([ 4, 14, 79, 80,  8, 81,  9, 82,  5,  3]),\n",
       "  tensor([ 4, 25, 26, 83,  8, 46, 47, 35, 84, 48, 10,  3]),\n",
       "  tensor([ 4, 85,  6, 36, 12, 86, 87, 88, 89, 27, 90, 91,  5,  3]),\n",
       "  tensor([ 4, 17, 14, 49, 37, 22, 92,  5,  3]),\n",
       "  tensor([ 4, 93, 50,  7, 11, 94,  6, 95, 18, 10,  3]),\n",
       "  tensor([  4,  18,  12,  28,  96,  35,  11,  51,  16,  97,   7,  98,  29,  99,\n",
       "          100,  52,  11,  51,  16, 101,   5,   3]),\n",
       "  tensor([  4,  18,  12,  38, 102,  30,  15, 103,  16,  23, 104,  39,  13,  39,\n",
       "            5,   3]),\n",
       "  tensor([  4,  40,   6,  20,   6, 105,  20,  36,  41, 106,   6, 107,   5,   3]),\n",
       "  tensor([  4,  38,  30,   5, 108,  48, 109,   6, 110,  12, 111,  31,  32,  53,\n",
       "          112,  13,  29, 113,  31, 114,  29,  54,  15, 115,   5,   3]),\n",
       "  tensor([  4,  15,   9, 116,   6,  20, 117, 118,   9, 119,  55,   7, 120, 121,\n",
       "          122,   7, 123,   7,  56,  23, 124,   5,   3]),\n",
       "  tensor([  4,  17,  14,  17,  14,  32, 125,  18,   5,   3]),\n",
       "  tensor([  4,  20,  31,  55, 126, 127, 128,  15, 129, 130,  27,  21,  57,   5,\n",
       "            3]),\n",
       "  tensor([  4,  38, 131, 132, 133,   9, 134,  24, 135, 136,   5,   3]),\n",
       "  tensor([  4, 137,  31, 138,  15,  39,  58, 139,   6,  13, 140,   3]),\n",
       "  tensor([  4, 141,   6,  33,  11, 142,  59,  32, 143, 144, 145,  10,   3]),\n",
       "  tensor([  4,  14, 146, 147,  60,  57,   5,   3]),\n",
       "  tensor([  4,  17,  18,  12, 148,  19, 149,  11, 150,   5,   3]),\n",
       "  tensor([  4,  22, 151,   8, 152,  42,   7,  43,   6, 153,  10,   3]),\n",
       "  tensor([  4,  61,   8,  62,  11, 154, 155, 156,  10,   3]),\n",
       "  tensor([  4,  34,   9,  58, 157, 158,  63, 159,   6,  20,  63, 160, 161,  10,\n",
       "            3]),\n",
       "  tensor([  4,  19,  64, 162, 163,  15, 164,   5,   3]),\n",
       "  tensor([  4,   8,  46,  64, 165, 166,   7, 167,   9, 168,   5,   3]),\n",
       "  tensor([  4,  44, 169, 170,   8, 171,   8,  65,   7,  37, 172,  41,   5,   3]),\n",
       "  tensor([  4,  13,  66,   8, 173,  43,   5,   5,   5,   3]),\n",
       "  tensor([  4,   8,  36,  42, 174, 175,  23,  54,   5,   3]),\n",
       "  tensor([  4,  40,   6,  19, 176, 177,  37,  18,  13,  19, 178,  28,  67,   5,\n",
       "            3]),\n",
       "  tensor([  4,  14,  49, 179,   9, 180,   6, 181,   5,   3]),\n",
       "  tensor([  4,   8,  65,   7, 182, 183, 184,  21, 185,  10,   3]),\n",
       "  tensor([  4,   8,  62, 186,  42,   6, 187,   5,   3]),\n",
       "  tensor([  4,  25,  26, 188, 189,   5,   3]),\n",
       "  tensor([  4, 190,  28,   7, 191,  27,  41,   5,   3]),\n",
       "  tensor([  4,  17,  22,  33,  68,  43,  21,  12,  34, 192,  21,   8, 193, 194,\n",
       "            7, 195,  10,   3]),\n",
       "  tensor([  4, 196, 197,   6,   8, 198, 199,   8,  24,   9, 200,  16, 201,   5,\n",
       "            3]),\n",
       "  tensor([  4,  19, 202,   7, 203,  12,  67,  25, 204,  13, 205,   5,   5,   5,\n",
       "           29, 206,  16, 207,   5,   3]),\n",
       "  tensor([  4,  68, 208,  69, 209,  13,  69, 210,  12,   5,   3]),\n",
       "  tensor([  4,  35,  11, 211,   6, 212, 213,   7, 214, 215, 216, 217,  40,   6,\n",
       "          218,   5,  14, 219,  11, 220,   7,  32,   5,   5,   5, 221,   9, 222,\n",
       "            5,   3]),\n",
       "  tensor([  4,  17, 223,  19,  10,  17, 224,   6, 225,   5,   3]),\n",
       "  tensor([  4, 226,  33,   9, 227,  13, 228,   3]),\n",
       "  tensor([  4,  18,  26,   9, 229,   6,  30,  10, 230,   6,  22,  12,   9, 231,\n",
       "          232,  10,   3]),\n",
       "  tensor([  4, 233,   6, 234,  66,  19,  33,  25,  30,   6,   8,  61,  53,  50,\n",
       "           16, 235,   5,   3]),\n",
       "  tensor([  4,  14, 236, 237, 238,   7, 239,   9, 240,  16, 241, 242, 243,   7,\n",
       "          244, 245,  27, 246,   5,   3]),\n",
       "  tensor([  4, 247,  13, 248, 249, 250,   6, 251,   5,   3]),\n",
       "  tensor([  4, 252,  15, 253,  13, 254,   5,   3]),\n",
       "  tensor([  4,  17, 255,   6, 256,   7, 257,   9, 258,   5,   3]),\n",
       "  tensor([  4, 259, 260, 261,  23, 262,  12, 263,  60,  10,   3]),\n",
       "  tensor([  4, 264,  11,  59,  26,  45, 265, 266,   6, 267,  24,  28,  22,  20,\n",
       "          268,   5,   3]),\n",
       "  tensor([  4,  23, 269, 270, 271, 272,  47, 273, 274, 275,   6,  56,   6, 276,\n",
       "          277, 278,   7,   8,  52,  11, 279, 280,  15,  11, 281, 282,   5,   3])],\n",
       " 'vi_ids': [tensor([  5,   7,  98,  99, 100, 101,  38,  56,   4,   3]),\n",
       "  tensor([  5, 102, 103, 104,  19, 105,  39,  57,   4,   3]),\n",
       "  tensor([  5, 106,  40,  22,   8,  23, 107, 108, 109, 110, 111,   4,   3]),\n",
       "  tensor([  5,   9,  58,  59,  60, 112, 113,  61,  62,  16, 114, 115,   4,   3]),\n",
       "  tensor([  5,  24,  11,  63,  25, 116,  64,  41,  10,   3]),\n",
       "  tensor([  5,  26,  14,  22,  19,  65, 117,  66,  42, 118, 119, 120, 121,   3]),\n",
       "  tensor([ 5, 27, 20,  8, 43, 44, 21, 11, 67, 28,  3]),\n",
       "  tensor([  5,  15, 122,   6,  29,   8,  10,   3]),\n",
       "  tensor([  5,   8, 123,  45,  15,  20,  68,  30, 124, 125,  69, 126, 127, 128,\n",
       "          129,  46,  20,  68,  30,  42, 130, 131,   4,   3]),\n",
       "  tensor([  5,  70,  47,  48,  67,  28,  46, 132,   9,   4,   3]),\n",
       "  tensor([  5,  16,   6, 133,   4,   4,   4,  16,  31, 134,  19,   4,   3]),\n",
       "  tensor([  5,  17,  71,   6, 135,  64,  46,   6, 136, 137, 138,  72,   4,   4,\n",
       "            4,  12,  32,  72, 139,  11,  18,  73,  74, 140, 141,   9,  33,   3]),\n",
       "  tensor([  5,  49, 142, 143,   6,  16,  45,  43,  75,   8,  34, 144, 145,  50,\n",
       "          146, 147, 148, 149,   4,   3]),\n",
       "  tensor([  5,  27, 150, 151, 152,  51, 153,   4,   4,   4,   3]),\n",
       "  tensor([  5,  16,  76,  77, 154, 155,  78, 156,  51, 157, 158, 159,  57,   4,\n",
       "            3]),\n",
       "  tensor([  5, 160, 161,  79, 162, 163, 164,  66,  20, 165,  11,  17, 166,   4,\n",
       "            3]),\n",
       "  tensor([  5, 167, 168, 169,  80,  13,   8,   7,  21,   6,  12,  13,   4,   4,\n",
       "            4,   3]),\n",
       "  tensor([  5, 170, 171,   7,  81, 172, 173,  82, 174,  25,   8, 175,  10,   3]),\n",
       "  tensor([  5,  61, 176, 177, 178,   4,   3]),\n",
       "  tensor([  5,  27,  25,  35,  60,  31,  26,  14, 179,  83,   4,   3]),\n",
       "  tensor([  5,   9,   7,  84,  69,  21,   8,   6, 180,  10,   3]),\n",
       "  tensor([  5,  52,   7,  36,  18, 181,  38,  56, 182,  10,   3]),\n",
       "  tensor([  5,  85, 183,  81, 184, 185, 186,   6,  16,  86, 187,  87,  53,  41,\n",
       "           10,   3]),\n",
       "  tensor([  5,  14,   7, 188, 189, 190, 191,  49,  88, 192,  37,   4,   3]),\n",
       "  tensor([  5, 193,   6,   9,   7, 194, 195,  31,  89, 196, 197,  28,   4,   3]),\n",
       "  tensor([ 5, 52, 33, 22, 23, 70, 44, 54,  9, 90, 34, 43,  4,  3]),\n",
       "  tensor([  5,  12,  75,   9,  91,  48,  19, 198, 199,  90,   9,   4,   4,   4,\n",
       "            3]),\n",
       "  tensor([ 5, 24, 31, 13, 92, 18, 24, 73, 74,  4,  3]),\n",
       "  tensor([  5, 200,  50,  26,  14,  58,  55,   6,  54,  91,  93,  17,  94,  55,\n",
       "            3]),\n",
       "  tensor([  5,  84,  13,   8,  29, 201,   6, 202,   4,   3]),\n",
       "  tensor([  5, 203,   7,  34, 204, 205,  38, 206, 207,  37,   8,  10,   3]),\n",
       "  tensor([  5,   9,  11, 208,  13,  71,   6, 209,   4,   3]),\n",
       "  tensor([  5,  25,  35,  15, 210, 211, 212, 213,   4,  82, 214,  83,   4,   3]),\n",
       "  tensor([  5, 215,  32,   7,  36, 216,  89, 217, 218, 219, 220,  37,   4,   3]),\n",
       "  tensor([  5, 221,  33,  11,  18,  21, 222,  32, 223, 224,  23,  44,  85,  10,\n",
       "            3]),\n",
       "  tensor([  5, 225, 226,  48, 227,  40,  35,  52, 228,  63, 229, 230,   4,   3]),\n",
       "  tensor([  5, 231,  80,  26,  13,  11, 232,  55, 233,  12, 234, 235,   4,   4,\n",
       "            4,   4,  39, 236, 237,   3]),\n",
       "  tensor([  5,  32,  33,  11,  95, 238, 239,  13,  12,  20, 240,  13,   4,   3]),\n",
       "  tensor([  5, 241,   6, 242,  47,  22, 243,  15, 244,   6,  13,  11, 245,  28,\n",
       "           15,  15, 246, 247,   4,   4,   4,   3]),\n",
       "  tensor([  5, 248,  14,  19,  17,  93,  10,   3]),\n",
       "  tensor([  5,  18,  41,  54, 249, 250, 251, 252,  30,   3]),\n",
       "  tensor([  5,  14,  86,  23,  94,  39,  65, 253, 254,  10,   3]),\n",
       "  tensor([  5,   9,   7,  36,   8,  29, 255, 256, 257,   4,   3]),\n",
       "  tensor([  5,  13, 258,  59, 259,   4,   4,   4,  12,  76,  77, 260, 261,  79,\n",
       "          262, 263, 264, 265, 266,   4,   3]),\n",
       "  tensor([  5, 267,   6,   9,  51,  96, 268,  12,  96,  18,  97,   3]),\n",
       "  tensor([  5,  40, 269,  49, 270,  12, 271,   4,   3]),\n",
       "  tensor([  5,  27, 272,   6, 273,  34,  95, 274, 275, 276,   4,   3]),\n",
       "  tensor([  5,  42, 277, 278,  37,   7,  92,  21,  45,  24, 279, 280, 281,  30,\n",
       "          282,   8,  10,   3]),\n",
       "  tensor([  5,  62,  15, 283,  35, 284, 285,   6,  50, 286, 287,   8,  29,  47,\n",
       "            9,  14, 288,   4,   3]),\n",
       "  tensor([  5, 289,  53, 290, 291, 292,  11,  17,  87, 293,  88,   4,   4,   4,\n",
       "           78,  53, 294,   6, 295,  12,   7,  36, 296, 297,   4,   4,   4,   7,\n",
       "           17, 298, 299, 300, 301, 302,  12,  97, 303, 304, 305, 306,   4,   3])]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_numericalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is to create the data loaders. These can be iterated upon to return a batch of data, each batch being a dictionary containing the numericalized English and Vietnamese sentences (which have also been padded) as PyTorch tensors.\n",
    "\n",
    "First, we need to create a function that collates, i.e. combines, a batch of examples into a batch. The collate_fn below takes a \"batch\" as input (a list of examples), we then separate out the English and German indices for each example in the batch, and pass each one to the pad_sequence function. pad_sequence takes a list of tensors, pads each one to the length of the longest tensor using the padding_value (which we set to pad_index, the index of our <pad> token) and then returns a [max length, batch size] shaped tensor, where batch size is the number of examples in the batch and max length is the length of the longest tensor in the batch. We put each tensor into a dictionary and then return it.\n",
    "\n",
    "The get_collate_fn takes in the padding token index and returns the collate_fn defined inside it. This technique, of defining a function inside another and returning it, is known as a closure. It allows the collate_fn to continually use the value of pad_index it was created with without creating a class or using global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the functions which give us our data loaders creating using PyTorch's DataLoader class.\n",
    "\n",
    "get_data_loader is created using a Dataset, the batch size, the padding token index (which is used for creating the batches in the collate_fn, and a boolean deciding if the examples should be shuffled at the time the data loader is iterated over.\n",
    "\n",
    "The batch size defines the maximum amount of examples within a batch. If the length of the dataset is not evenly divisible by the batch size then the last batch will be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our data loaders.\n",
    "\n",
    "To reduce the training time, we generally want to us the largest batch size possible. When using a GPU, this means using the largest batch size that will fit in GPU memory.\n",
    "\n",
    "Shuffling of data makes training more stable and potentially improves the final performance of the model, however only needs to be done on the training set. The metrics calculated for the validation and test set will be the same no matter what order the data is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "pad_index = en_vocab.get_index('<pad>')\n",
    "\n",
    "train_data_loader = get_data_loader(train_data_numericalized, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data_numericalized, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data_numericalized, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ec16fa1a30>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
